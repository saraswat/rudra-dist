# 
# cifar10.cfg
# specifies the meta parameters for mlp training
{
trainData       = /home/jjmiltho/rudra_data/CIFAR10_bin/CIFAR10_trainImagesOrig_meanSub.bin
trainLabels     = /home/jjmiltho/rudra_data/CIFAR10_bin/CIFAR10_trainLabels.bin
testData        = /home/jjmiltho/rudra_data/CIFAR10_bin/CIFAR10_testImagesOrig_meanSub.bin
testLabels      = /home/jjmiltho/rudra_data/CIFAR10_bin/CIFAR10_testLabels.bin
layerCfgFile	= /home/jjmiltho/rudra-dist/examples/cifar10_full.cnn

checkpointInterval = 0

numTrainSamples = 50000
numTestSamples 	= 10000
numInputDim	= 3072
numClasses     	= 10
numEpochs	= 3
batchSize	= 16
alphaDecay	= 1

# piecewise constant learning rate schedule
# reduce learning rate by factor 10 after 120th and 130th epoch

learningSchedule = step
epochs          = 120,130
gamma           = 0.1
}


